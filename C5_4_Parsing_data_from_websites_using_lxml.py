# C5.4. Парсинг данных с сайтов с помощью lxml

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Парсинг сайтов на примере Python.org
# В сегодняшнем юните научимся писать простенькие парсеры.
#
# Парсеры — это специальные программы, которые позволяют собирать информацию
# с веб-сайтов, не заходя на них через браузер.
# То есть, например, если вы захотели составить базу данных товара какого-либо
# интернет-магазина, то вам не обязательно перемещаться по нему и вручную отбирать
# все названия, фото товара и ссылки на сам товар. Для этого достаточно написать парсер,
# который по определенным отличительным признакам в HTML-коде (как правило, это
# классы или id) будет находить вам нужную информацию.
#
# Конкретно в этом туториале мы будем пользоваться библиотекой lxml для парсинга
# данных. Но перед тем как писать парсеры, предлагаем вам посмотреть небольшое
# вступление для лучшего понимания того, с чем мы будем иметь дело.
#
#
# Теперь давайте для начала установим саму библиотеку:
#
# pip3 install lxml.
#
# Теперь перейдем в редактор и напишем следующий код:
#
# import requests  # импортируем наш знакомый модуль
# import lxml.html
# from lxml import etree
#
# html = requests.get('https://www.python.org/').content  # получим html главной странички официального сайта python
#
# tree = lxml.html.document_fromstring(html)
# title = tree.xpath('/html/head/title/text()')  # забираем текст тега <title> из тега <head>, который лежит в свою очередь внутри тега <html> (в этом невидимом теге <head> у нас хранится основная информация о страничке. Её название и инструкции по отображению в браузере.
#
# print(title)  # выводим полученный заголовок страницы
# В результате в консоль у нас выведется:
#
# Обратили внимание, что это за тип данных? Да-да, это список. Потому как lxml
# находит и возвращает все элементы, удовлетворяющие заданным условиям, даже,
# если такой элемент всего один, он также будет являться частью списка.
#
# Возможно вас напугала строчка, переданная в  функцию xpath. Если вам лень
# вдумываться и писать параметр в xpath вручную, можно воспользоваться функцией
# браузера и просто скопировать xpath до нужного вам элемента.
#
# Давайте теперь попробуем собрать заголовки всех новостей с Python.org. Но для этого
# сейчас мы рассмотрим немного другой подход парсинга. Сохраним HTML-страничку
# и поместим её в папку со скриптом, так как метод, который мы рассмотрим дальше,
# будет работать с HTML-файлом.
#
# Нажимаем правой кнопкой мыши по страничке в браузере и выбираем «Сохранить как».
# Сохраняем в папку с нашим скриптом.
# Сначала скопируем путь до заголовка, чтобы не писать его вручную.
#
# Тот самый список новостей, который нас будет интересовать.
# Это делается следующим образом:
#
# В консоли надо выбрать нужный вам элемент (нажимаем правой кнопкой мыши
# по заголовку, далее нажимаем «Исследовать элемент», после чего нужный элемент
# в HTML выберется в консоли автоматически) и нажимаем по нему правой кнопкой
# мыши. Переходим в пункт копирования.
#
#
# Выбрать «Копировать XPath».
#
# Ну и вот! Всё готово! Нужный путь скопирован у нас в буфер обмена!
#
import requests  # импортируем наш знакомый модуль
# import lxml.html
# from lxml import etree
#
# # создадим объект ElementTree. Он возвращается функцией parse()
# tree = etree.parse(f'Welcome to Python.org.html', lxml.html.HTMLParser())  # попытаемся спарсить наш файл с помощью html-парсера. Сам html - это то, что мы скачали и поместили в папку из браузера.
#
# ul = tree.findall('/html/body/div/div[3]/div/section/div[2]/div[1]/div/ul/li[1]/a')  # помещаем в аргумент метода findall скопированный xpath. Здесь мы получим все элементы списка новостей. (Все заголовки и их даты)
#
# # создаём цикл, в котором мы будем выводить название каждого элемента из списка
# for li in ul:
#     a = li.find(
#         'a')  # в каждом элементе находим, где хранится заголовок новости. У нас это тег <a>. Т. е. гиперссылка, на которую нужно нажать, чтобы перейти на страницу с новостью. (Гиперссылки в html это всегда тэг <a>)
#     print(a.text)  # из этого тега забираем текст, это и будет нашим названием

# из этого тега забираем текст, это и будет нашим названием
# Обратите внимание, что в скопированном из браузера xpath надо внести изменения.
# А конкретно: мы удалили начальный тег /HTML из поиска. В основном методы
# find и findall работают так же, как и функция xpath, но всё же есть отличия. Как вы
# догадались, findall возвращает список многих подходящих элементов, в то время
# как метод find возвращает только первый подходящий элемент. Также второй
# аргумент в функции .parse() обязательный. Без него мы парсить не сможем,
# потому как для восприятия парсером переданного в IDE HTML-текста, а не
# какого-либо ещё, нужно передать объект класса HTMLParser.
#
# Теперь взглянем в консоль.
#
# Вывелись все наши заголовки! Как мы и хотели.
#
# Ознакомившись с кратким гайдом, ответьте на следующие вопросы.
#
# 5.4.1
#
# Представлена строка: <a href="/add">. Тегом в ней является:
#
# Ответ: a
#
# 5.4.2
#
# Представлена строка: <a href="/add">. Атрибутом в ней является:
#
# Ответ: href
#
# 5.4.3
#
# Напишите xpath для нахождения текста элемента tag2. Помните, что мы идём от
# самого верхнего тега к самому нижнему, разделяя их через /.
#
# <HTML>
#  <body>
#   <tag1> some text
#      <tag2> MY TEXT </tag2>
#    </tag1>
#  </body>
# </HTML>
#
# Ответ: /HTML/body/tag1/tag2/text()
#
# 5.4.4
#
# Напишите программу, которая будет с помощью парсера lxml доставать текст из
# тега tag2 следующего HTML:
#
# <html>
#  <head> <title> Some title </title> </head>
#  <body>
#   <tag1> some text
#      <tag2> MY TEXT </tag2>
#    </tag1>
#  </body>
# </html>
#
# Решение:
#
# import lxml.html
#
#
# html = ''' <html>
#  <head> <title> Some title </title> </head>
#  <body>
#   <tag1> some text
#      <tag2> MY TEXT </tag2>
#    </tag1>
#  </body>
# </html>
# '''
#
# tree = lxml.html.document_fromstring(html)
#
# text = tree.xpath('/html/body/tag1/tag2/text()')
#
# print(text)
#
# 5.4.5
#
# Используя полученные знания, допишите сделанный в начале юнита скрипт
# (где мы доставали заголовки новостей о Python с Python.org) так, чтобы он
# показывал ещё и дату добавления новости.
#
# Примечание: Для получения атрибутов тега (т. е. его дополнительных параметров)
# используется метод .get(<имя атрибута>).
#
# Решение:
#
# import requests # импортируем наш знакомый модуль
# import lxml.html
# from lxml import etree
#
# html = requests.get('https://www.python.org/').content # получим html главной странички официального сайта python
#
# # создадим объект ElementTree. Он возвращается функцией parse()
# tree = etree.parse('Welcome to Python.org.html', lxml.html.HTMLParser()) # попытаемся спарсить наш файл с помощью html парсера
#
# ul = tree.findall('body/div/div[3]/div/section/div[3]/div[1]/div/ul/li') # помещаем в аргумент методу findall скопированный xpath
#
# # создаём цикл в котором мы будем выводить название каждого элемента из списка
# for li in ul:
#     a = li.find('a') # в каждом элементе находим где хранится название. У нас это тег <a>
#     time = li.find('time')
#     print(time.get('datetime'), a.text) # из этого тега забираем текст - это и будет нашим названием
# Если вас заинтересовала тема парсинга, то вы можете найти ещё немного
# о ней в дополнительных материалах. Там мы разбираем библиотеку Beautiful Soup и
# её использование для парсинга. Делает она почти тоже самое, просто с ней работать
# немножечко легче. Материал полностью опциональный, но всё же рекомендуется
# к ознакомлению. :)